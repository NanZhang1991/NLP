{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas scikit-learn matplotlib tensorflow transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from ast import literal_eval\n",
    "from typing import Dict, Optional, Tuple, Union\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from transformers import TFBertPreTrainedModel, TFBertMainLayer, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Nan\\AppData\\Local\\Temp\\ipykernel_15704\\4070111792.py:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "True\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(tf.test.is_gpu_available())\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"./data/MultilabelSequenceClassification/toxic-comment-classification/train.csv.zip\"\n",
    "test_path = \"./data/MultilabelSequenceClassification/toxic-comment-classification/test.csv.zip\"\n",
    "\n",
    "# df = pd.read_csv(train_path)\n",
    "# df['label'] = df[df.columns[2:]].values.tolist()\n",
    "# new_df = df[['comment_text', 'label']].copy()\n",
    "# new_df.rename(columns={'comment_text':'content'}, inplace=True)\n",
    "# train_size=0.9\n",
    "# test_data = pd.read_csv(test_path)[:1000]\n",
    "# train_data = new_df.sample(frac=train_size,random_state=200)[:1000]\n",
    "# val_data = new_df.drop(train_data.index).reset_index(drop=True)[:1000]\n",
    "\n",
    "# print(\"FULL Dataset: {}\".format(new_df.shape))\n",
    "# print(\"TRAIN Dataset: {}\".format(train_data.shape))\n",
    "# print(\"VALIDATION Dataset: {}\".format(val_data.shape))\n",
    "# print(\"TEST Dataset: {}\".format(test_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chineses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (100, 2)\n",
      "TRAIN Dataset: (90, 2)\n",
      "VALIDATION Dataset: (10, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>好消息！国足对手叙利亚面临换帅足协高层已经集体辞职了</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>随着里贝里、罗本赛季结束后离开，长期为球迷奉献精彩比赛的拜仁“罗贝里”黄金组合也将结束历史使命。</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>继王晨艺退赛后创2陆思恒又被爆出黑料！网友“又挡了谁的路”？</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>9月11日，第十一届全国少数民族传统体育运动会女子传统拳术三类决赛展开较量。来自山东武术院的...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>蔚来召回ES8，而特斯拉只有傲慢，网友：欺负我们没见过世面</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              content  \\\n",
       "60                         好消息！国足对手叙利亚面临换帅足协高层已经集体辞职了   \n",
       "6    随着里贝里、罗本赛季结束后离开，长期为球迷奉献精彩比赛的拜仁“罗贝里”黄金组合也将结束历史使命。   \n",
       "17                     继王晨艺退赛后创2陆思恒又被爆出黑料！网友“又挡了谁的路”？   \n",
       "43  9月11日，第十一届全国少数民族传统体育运动会女子传统拳术三类决赛展开较量。来自山东武术院的...   \n",
       "62                      蔚来召回ES8，而特斯拉只有傲慢，网友：欺负我们没见过世面   \n",
       "\n",
       "                                                label  \n",
       "60  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "6   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "17  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "43  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "62  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_dataset(train_path, train_size=0.9):\n",
    "    df = pd.read_csv(train_path)\n",
    "    new_df = df[['content', 'label_ids']].copy()\n",
    "    new_df.rename(columns={'label_ids':'label'}, inplace=True)\n",
    "    new_df.label = new_df.label.apply(literal_eval)\n",
    "    train_data = new_df.sample(frac=train_size, random_state=200)\n",
    "    val_data = new_df.drop(train_data.index)\n",
    "\n",
    "    train_data.reset_index(drop=True, inplace=True)\n",
    "    val_data.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    print(f\"FULL Dataset: {new_df.shape}\")\n",
    "    print(f\"TRAIN Dataset: {train_data.shape}\")\n",
    "    print(f\"VALIDATION Dataset: {val_data.shape}\")\n",
    "\n",
    "    return train_data, val_data\n",
    "\n",
    "train_path = \"./data/MultilabelSequenceClassification/chinese_dataset/train_dataset.zip\"\n",
    "train_data, val_data = load_dataset(train_path=train_path)\n",
    "train_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional, Tuple, Union\n",
    "from transformers.modeling_tf_utils import get_initializer, TFModelInputType, TFSequenceClassificationLoss\n",
    "from transformers.modeling_tf_outputs import TFSequenceClassifierOutput\n",
    "from transformers.tf_utils import shape_list\n",
    "\n",
    "class TFSequenceClassificationLoss:\n",
    "    \"\"\"\n",
    "    Loss function suitable for sequence classification.\n",
    "    \"\"\"\n",
    "\n",
    "    def hf_compute_loss(self, labels, logits):\n",
    "        if len(shape_list(logits)) == 1 or shape_list(logits)[1] == 1:\n",
    "            loss_fn = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)\n",
    "        else:\n",
    "            loss_fn = tf.keras.losses.BinaryCrossentropy(\n",
    "                from_logits=True, reduction=tf.keras.losses.Reduction.NONE\n",
    "            )\n",
    "\n",
    "        return loss_fn(labels, logits)\n",
    "\n",
    "class TFBertForMultilabelClassification(TFBertPreTrainedModel, TFSequenceClassificationLoss):\n",
    "\n",
    "    def __init__(self, config, *inputs, **kwargs):\n",
    "        super(TFBertForMultilabelClassification, self).__init__(config, *inputs, **kwargs)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.bert = TFBertMainLayer(config, name='bert')\n",
    "        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = tf.keras.layers.Dense(config.num_labels,\n",
    "                                                kernel_initializer=get_initializer(config.initializer_range),\n",
    "                                                name='classifier')\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        input_ids: Optional[TFModelInputType] = None,\n",
    "        attention_mask: Optional[Union[np.ndarray, tf.Tensor]] = None,\n",
    "        token_type_ids: Optional[Union[np.ndarray, tf.Tensor]] = None,\n",
    "        position_ids: Optional[Union[np.ndarray, tf.Tensor]] = None,\n",
    "        head_mask: Optional[Union[np.ndarray, tf.Tensor]] = None,\n",
    "        inputs_embeds: Optional[Union[np.ndarray, tf.Tensor]] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        labels: Optional[Union[np.ndarray, tf.Tensor]] = None,\n",
    "        training: Optional[bool] = False,\n",
    "    ) -> Union[TFSequenceClassifierOutput, Tuple[tf.Tensor]]:\n",
    "        r\"\"\"\n",
    "        labels (`tf.Tensor` or `np.ndarray` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            training=training,\n",
    "        )\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(inputs=pooled_output, training=training)\n",
    "        logits = self.classifier(inputs=pooled_output)\n",
    "        loss = None if labels is None else self.hf_compute_loss(labels=labels, logits=logits)\n",
    "\n",
    "        # if not return_dict:\n",
    "        #     output = (logits,) + outputs[2:]\n",
    "        #     return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return TFSequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customDataset(dataset, tokenizer, max_length):\n",
    "    inputs = tokenizer(dataset['content'].tolist(), max_length=max_length, padding='max_length', truncation=True,\\\n",
    "                   return_tensors='tf')\n",
    "    if 'label' in dataset.columns:\n",
    "        label_list = dataset['label'].values.tolist() \n",
    "    else:\n",
    "        label_list = None\n",
    "    result = tf.data.Dataset.from_tensor_slices((dict((k,v) for k, v in inputs.items()), label_list))          \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English\n",
    "# model_path = '../models/bert-base-uncased'\n",
    "# Chinese\n",
    "model_path = '../models/bert-base-chinese'\n",
    "# parameters\n",
    "max_length = 128\n",
    "batch_size = 8\n",
    "learning_rate = 1e-5\n",
    "num_epochs = 5\n",
    "# num_classes = 6 \n",
    "num_classes = 65\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "\n",
    "ds_train_encoded = customDataset(train_data, tokenizer, max_length).shuffle(100).batch(batch_size)\n",
    "ds_val_encoded = customDataset(val_data, tokenizer, max_length).batch(batch_size)\n",
    "# ds_test_encoded = customDataset(test_data, tokenizer, max_length).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ../models/bert-base-chinese were not used when initializing TFBertForMultilabelClassification: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertForMultilabelClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForMultilabelClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFBertForMultilabelClassification were not initialized from the model checkpoint at ../models/bert-base-chinese and are newly initialized: ['dropout_37', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# model initialization\n",
    "model = TFBertForMultilabelClassification.from_pretrained(model_path, num_labels=num_classes)#------------6个标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (data, label) in enumerate(ds_val_encoded):\n",
    "#     print(data, label)\n",
    "#     if i == 0:\n",
    "#         break\n",
    "# output = model(**data,  labels=label)\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1/9 [==>...........................] - ETA: 2:53 - loss: 0.6966 - categorical_accuracy: 0.0000e+00"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'tf_bert_for_multilabel_classification/bert/encoder/layer_._11/intermediate/Gelu/mul_1' defined at (most recent call last):\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\traitlets\\config\\application.py\", line 976, in launch_instance\n      app.start()\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\asyncio\\events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_cell\n      result = self._run_cell(\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2936, in _run_cell\n      return runner(coro)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3135, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3338, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3398, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Nan\\AppData\\Local\\Temp\\ipykernel_15704\\1921213348.py\", line 8, in <cell line: 8>\n      bert_history = model.fit(ds_train_encoded, epochs= num_epochs, validation_data=ds_val_encoded)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\keras\\engine\\training.py\", line 1409, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function\n      return step_function(self, iterator)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step\n      outputs = model.train_step(data)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1398, in train_step\n      y_pred = self(x, training=True)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\keras\\engine\\training.py\", line 490, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Nan\\AppData\\Local\\Temp\\ipykernel_15704\\1655751431.py\", line 52, in call\n      outputs = self.bert(\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 753, in run_call_with_unpacked_inputs\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 863, in call\n      encoder_outputs = self.encoder(\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 548, in call\n      for i, layer_module in enumerate(self.layer):\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 554, in call\n      layer_outputs = layer_module(\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 510, in call\n      intermediate_output = self.intermediate(hidden_states=attention_output)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 414, in call\n      hidden_states = self.intermediate_act_fn(hidden_states)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\keras\\activations.py\", line 351, in gelu\n      return tf.nn.gelu(x, approximate)\nNode: 'tf_bert_for_multilabel_classification/bert/encoder/layer_._11/intermediate/Gelu/mul_1'\nfailed to allocate memory\n\t [[{{node tf_bert_for_multilabel_classification/bert/encoder/layer_._11/intermediate/Gelu/mul_1}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_31233]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32me:\\NanZhang1991\\NLP\\sequence_classification\\tf2_bert_MultiLabel.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/NanZhang1991/NLP/sequence_classification/tf2_bert_MultiLabel.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39moptimizer, loss\u001b[39m=\u001b[39mloss, metrics\u001b[39m=\u001b[39m[metric])\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/NanZhang1991/NLP/sequence_classification/tf2_bert_MultiLabel.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# fit model\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/NanZhang1991/NLP/sequence_classification/tf2_bert_MultiLabel.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m bert_history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(ds_train_encoded, epochs\u001b[39m=\u001b[39;49m num_epochs, validation_data\u001b[39m=\u001b[39;49mds_val_encoded)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/NanZhang1991/NLP/sequence_classification/tf2_bert_MultiLabel.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m model\u001b[39m.\u001b[39mevaluate(ds_val_encoded)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/NanZhang1991/NLP/sequence_classification/tf2_bert_MultiLabel.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m model\u001b[39m.\u001b[39msave_pretrained(\u001b[39m'\u001b[39m\u001b[39m../models/fine_tune_multiLable_model/\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32md:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32md:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'tf_bert_for_multilabel_classification/bert/encoder/layer_._11/intermediate/Gelu/mul_1' defined at (most recent call last):\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\traitlets\\config\\application.py\", line 976, in launch_instance\n      app.start()\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\asyncio\\events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_cell\n      result = self._run_cell(\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2936, in _run_cell\n      return runner(coro)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3135, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3338, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3398, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Nan\\AppData\\Local\\Temp\\ipykernel_15704\\1921213348.py\", line 8, in <cell line: 8>\n      bert_history = model.fit(ds_train_encoded, epochs= num_epochs, validation_data=ds_val_encoded)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\keras\\engine\\training.py\", line 1409, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function\n      return step_function(self, iterator)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step\n      outputs = model.train_step(data)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1398, in train_step\n      y_pred = self(x, training=True)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\keras\\engine\\training.py\", line 490, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Nan\\AppData\\Local\\Temp\\ipykernel_15704\\1655751431.py\", line 52, in call\n      outputs = self.bert(\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 753, in run_call_with_unpacked_inputs\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 863, in call\n      encoder_outputs = self.encoder(\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 548, in call\n      for i, layer_module in enumerate(self.layer):\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 554, in call\n      layer_outputs = layer_module(\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 510, in call\n      intermediate_output = self.intermediate(hidden_states=attention_output)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 414, in call\n      hidden_states = self.intermediate_act_fn(hidden_states)\n    File \"d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\keras\\activations.py\", line 351, in gelu\n      return tf.nn.gelu(x, approximate)\nNode: 'tf_bert_for_multilabel_classification/bert/encoder/layer_._11/intermediate/Gelu/mul_1'\nfailed to allocate memory\n\t [[{{node tf_bert_for_multilabel_classification/bert/encoder/layer_._11/intermediate/Gelu/mul_1}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_31233]"
     ]
    }
   ],
   "source": [
    "# optimizer Adam recommended\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5,epsilon=1e-08, clipnorm=1)\n",
    "# we do not have one-hot vectors, we can use sparce categorical cross entropy and accuracy\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)# binary_crossentropy 损失函数\n",
    "metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "# fit model\n",
    "bert_history = model.fit(ds_train_encoded, epochs= num_epochs, validation_data=ds_val_encoded)\n",
    "model.evaluate(ds_val_encoded)\n",
    "model.save_pretrained('../models/fine_tune_multiLable_model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate val_set\n",
    "# pred=model.predict(ds_val_encoded)\n",
    "# pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(epoch):\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    for _, data in enumerate(ds_val_encoded, 0):\n",
    "        inputs = data[0]\n",
    "        targets = data[1]\n",
    "        outputs = model(inputs)[0]\n",
    "        fin_targets.extend(targets.numpy().tolist())\n",
    "        fin_outputs.extend(outputs.numpy().tolist())\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score = 0.0\n",
      "precision (Micro) = 0.0\n",
      "precision (Macro) = 0.0\n",
      "precision (samples) = 0.0\n",
      "recall_score (Micro) = 0.0\n",
      "recall_score (Macro) = 0.0\n",
      "recall_score (samples) = 0.0\n",
      "F1 Score (Micro) = 0.0\n",
      "F1 Score (Macro) = 0.0\n",
      "F1 Score (samples) = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1599: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    }
   ],
   "source": [
    "outputs, targets = validation(num_epochs)\n",
    "targets = np.array(targets)\n",
    "outputs = np.array(outputs) >= 0.5\n",
    "\n",
    "accuracy = metrics.accuracy_score(targets, outputs)\n",
    "precision_micro = metrics.precision_score(targets, outputs, average='micro')\n",
    "precision_macro = metrics.precision_score(targets, outputs, average='macro')\n",
    "precision_samples = metrics.precision_score(targets, outputs, average='samples')\n",
    "recall_score_micro = metrics.recall_score(targets, outputs, average='micro')\n",
    "recall_score_macro = metrics.recall_score(targets, outputs, average='macro')\n",
    "recall_score_samples = metrics.recall_score(targets, outputs, average='samples')\n",
    "f1_score_micro = metrics.f1_score(targets, outputs, average='micro')\n",
    "f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\n",
    "f1_score_samples = metrics.f1_score(targets, outputs, average='samples')\n",
    "\n",
    "print(f\"Accuracy Score = {accuracy}\")\n",
    "print(f\"precision (Micro) = {precision_micro}\")\n",
    "print(f\"precision (Macro) = {precision_macro}\")\n",
    "print(f\"precision (samples) = {precision_samples}\")\n",
    "print(f\"recall_score (Micro) = {recall_score_micro}\")\n",
    "print(f\"recall_score (Macro) = {recall_score_macro}\")\n",
    "print(f\"recall_score (samples) = {recall_score_samples}\")\n",
    "print(f\"F1 Score (Micro) = {f1_score_micro}\")\n",
    "print(f\"F1 Score (Macro) = {f1_score_macro}\")\n",
    "print(f\"F1 Score (samples) = {f1_score_samples}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'kill' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
      "���������ļ���\n"
     ]
    }
   ],
   "source": [
    "pid = os.getpid()\n",
    "!kill -9 $pid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "c87b1f44594b67819a704c3c4238e15d055a646151ba29c648f9c587b7431b78"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

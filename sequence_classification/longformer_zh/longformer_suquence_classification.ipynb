{"cells":[{"cell_type":"code","execution_count":57,"metadata":{"colab":{"background_save":true},"id":"rKzflpZ9WNtl"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/gdrive/')"]},{"cell_type":"code","execution_count":58,"metadata":{"colab":{"background_save":true},"id":"FrDYpPMDWbz4"},"outputs":[],"source":["# import os\n","# os.chdir('/content/gdrive/MyDrive/Colab Notebooks/sequence_classification/longformer_zh')"]},{"cell_type":"code","execution_count":59,"metadata":{"colab":{"background_save":true},"id":"65bkWtKFWh0u"},"outputs":[{"name":"stdout","output_type":"stream","text":[" ������ E �еľ��� m2\n"," �������к��� 6C3B-1FD3\n","\n"," e:\\NanZhang1991\\NLP\\sequence_classification\\longformer_zh ��Ŀ¼\n","\n","2022/08/18  02:19    <DIR>          .\n","2022/08/14  12:13    <DIR>          ..\n","2022/08/18  00:12    <DIR>          .ipynb_checkpoints\n","2022/07/10  22:15                 4 __init__.py\n","2022/07/10  22:15            19,059 classification.py\n","2022/08/18  02:23    <DIR>          finetune_model\n","2022/08/14  12:13    <DIR>          longformer\n","2022/08/18  02:13             9,937 longformer.ipynb\n","2022/08/18  02:23            28,650 longformer_suquence_classification.ipynb\n","2022/07/10  22:15             4,308 predict.py\n","2022/07/10  22:15             5,152 README.md\n","2022/07/10  22:15                75 requirements.txt\n","2022/08/18  02:23    <DIR>          runs\n","2022/07/18  02:41             5,157 test.ipynb\n","               8 ���ļ�         72,342 �ֽ�\n","               6 ��Ŀ¼ 206,425,726,976 �����ֽ�\n"]}],"source":["ls"]},{"cell_type":"markdown","metadata":{"id":"a_SCDw2rVQtO"},"source":["## 导入环境"]},{"cell_type":"code","execution_count":60,"metadata":{"colab":{"background_save":true},"id":"kfsISrFXVQtU"},"outputs":[],"source":["# ! pip install numpy pandas torch transformers progressbar tqdm"]},{"cell_type":"code","execution_count":61,"metadata":{"id":"05uR6kQAVQtW"},"outputs":[],"source":["from pathlib import Path\n","from datetime import datetime\n","import pandas as pd\n","import json\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","from tqdm import tqdm\n","from progressbar import ProgressBar, Percentage, Bar, Timer, ETA, FileTransferSpeed\n","from transformers import BertTokenizer\n","from longformer.longformer import LongformerConfig, LongformerForSequenceClassification"]},{"cell_type":"markdown","metadata":{"id":"DxnX-DFLVQtZ"},"source":["## 构建数据集"]},{"cell_type":"markdown","metadata":{"id":"Tg8Qg__PVQta"},"source":["### 数据清洗"]},{"cell_type":"code","execution_count":62,"metadata":{"id":"vFYMPKGwVQtb"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>content</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>3746</th>\n","      <td>[司法行为-拘捕, 司法行为-罚款]</td>\n","      <td>“高三男生阻止插队被打骨折”打人男子被行拘十天罚款500元</td>\n","    </tr>\n","    <tr>\n","      <th>10943</th>\n","      <td>[竞赛行为-胜负, 竞赛行为-晋级]</td>\n","      <td>8月23日，在瑞士巴塞尔举行的2019年世界羽毛球锦标赛女子双打四分之一决赛中，中国组合杜玥...</td>\n","    </tr>\n","    <tr>\n","      <th>3445</th>\n","      <td>[司法行为-拘捕]</td>\n","      <td>近日，广东深圳龙岗警方组织警力前往宝安，南山及东莞市等地，抓获犯罪嫌疑人10人，成功破获这起...</td>\n","    </tr>\n","    <tr>\n","      <th>10579</th>\n","      <td>[竞赛行为-胜负]</td>\n","      <td>北京时间9月3日，男篮世界杯小组继续进行，在F组的一场比赛中，黑山迎战新西兰，结果他们以83...</td>\n","    </tr>\n","    <tr>\n","      <th>9401</th>\n","      <td>[财经/交易-涨价]</td>\n","      <td>16日布伦特原油价格一度创下28年以来最大日涨幅。</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                    label                                            content\n","3746   [司法行为-拘捕, 司法行为-罚款]                      “高三男生阻止插队被打骨折”打人男子被行拘十天罚款500元\n","10943  [竞赛行为-胜负, 竞赛行为-晋级]  8月23日，在瑞士巴塞尔举行的2019年世界羽毛球锦标赛女子双打四分之一决赛中，中国组合杜玥...\n","3445            [司法行为-拘捕]  近日，广东深圳龙岗警方组织警力前往宝安，南山及东莞市等地，抓获犯罪嫌疑人10人，成功破获这起...\n","10579           [竞赛行为-胜负]  北京时间9月3日，男篮世界杯小组继续进行，在F组的一场比赛中，黑山迎战新西兰，结果他们以83...\n","9401           [财经/交易-涨价]                          16日布伦特原油价格一度创下28年以来最大日涨幅。"]},"execution_count":62,"metadata":{},"output_type":"execute_result"}],"source":["fp = '../data/MultilabelSequenceClassification/chinese_dataset/train.zip'\n","train_df = pd.read_csv(fp).fillna(value='')\n","train_df.label = train_df.label.str.split('|')\n","train_df.sample(5)"]},{"cell_type":"markdown","metadata":{"id":"ATLiGt_CVQtc"},"source":["### 标签id"]},{"cell_type":"code","execution_count":63,"metadata":{"id":"0bUQqWP1VQtd"},"outputs":[{"data":{"text/plain":["65"]},"execution_count":63,"metadata":{},"output_type":"execute_result"}],"source":["\n","def id2label(label):\n","    labels = label.explode().drop_duplicates()\n","    id2label = dict(zip(range(len(labels)), labels))\n","    label2id = {v: k for k, v in id2label.items()}\n","\n","    with open(\"../data/MultilabelSequenceClassification/chinese_dataset/label.json\", \"w\", encoding=\"utf-8\") as f:\n","        f.write(json.dumps(id2label, ensure_ascii=False, indent=2))\n","    return id2label, label2id\n","    \n","id2label, label2id = id2label(train_df.label)\n","len(id2label)"]},{"cell_type":"code","execution_count":64,"metadata":{"id":"S4AWIjOZVQtf"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>content</th>\n","      <th>label_ids</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[组织关系-裁员]</td>\n","      <td>雀巢裁员4000人：时代抛弃你时，连招呼都不会打！</td>\n","      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[组织关系-裁员]</td>\n","      <td>美国“未来为”子公司大幅度裁员，这是为什么呢？任正非正式回应</td>\n","      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[组织关系-裁员]</td>\n","      <td>这一全球巨头“凉凉”“捅刀”华为后裁员5000现市值缩水800亿</td>\n","      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>[组织关系-裁员]</td>\n","      <td>被证实将再裁员1800人AT&amp;T在为落后的经营模式买单</td>\n","      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>[组织关系-裁员]</td>\n","      <td>又一网约车巨头倒下：三个月裁员835名员工，滴滴又该何去何从</td>\n","      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       label                           content  \\\n","0  [组织关系-裁员]         雀巢裁员4000人：时代抛弃你时，连招呼都不会打！   \n","1  [组织关系-裁员]    美国“未来为”子公司大幅度裁员，这是为什么呢？任正非正式回应   \n","2  [组织关系-裁员]  这一全球巨头“凉凉”“捅刀”华为后裁员5000现市值缩水800亿   \n","3  [组织关系-裁员]       被证实将再裁员1800人AT&T在为落后的经营模式买单   \n","4  [组织关系-裁员]    又一网约车巨头倒下：三个月裁员835名员工，滴滴又该何去何从   \n","\n","                                           label_ids  \n","0  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n","1  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n","2  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n","3  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n","4  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "]},"execution_count":64,"metadata":{},"output_type":"execute_result"}],"source":["def label_ids(label):\n","    label_ids = [0] * len(id2label)\n","    for separate_label in label:\n","        label_id = label2id.get(separate_label)\n","        if label_id is not None:\n","            label_ids[label_id] = 1\n","    return  label_ids\n","\n","train_df['label_ids'] = train_df.label.apply(label_ids)\n","train_df.to_csv('../data/MultilabelSequenceClassification/chinese_dataset/train_dataset.zip')\n","train_df.head()"]},{"cell_type":"markdown","metadata":{"id":"dil0GxRMVQti"},"source":["## 模型训练"]},{"cell_type":"markdown","metadata":{"id":"zz4OWMBwVQtj"},"source":["### 加载预训练模型"]},{"cell_type":"code","execution_count":65,"metadata":{"id":"bIGyer1vVQtk"},"outputs":[{"data":{"text/plain":["device(type='cuda')"]},"execution_count":65,"metadata":{},"output_type":"execute_result"}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# device = torch.device('cpu')\n","device"]},{"cell_type":"code","execution_count":66,"metadata":{"id":"JQu0ihB_VQtl"},"outputs":[],"source":["# model_path = Path(\"../../models/distilbert-base-uncased\")\n","# tokenizer = BertTokenizer.from_pretrained(model_path)\n","# config = LongformerConfig.from_pretrained(model_path)\n","# config.problem_type = \"multi_label_classification\"\n","# config.num_labels = 6\n","# model = BertForSequenceClassification.from_pretrained(model_path, config=config)\n","# model.to(device)"]},{"cell_type":"markdown","metadata":{"id":"yiEC3ZDdVQtn"},"source":["### 加载数据集"]},{"cell_type":"code","execution_count":67,"metadata":{"id":"yGVmBfEaVQtn"},"outputs":[],"source":["import pandas as pd \n","from ast import literal_eval\n","\n","def load_dataset(train_path, train_size=0.9):\n","    df = pd.read_csv(train_path)\n","    new_df = df[['content', 'label_ids']].copy()\n","    new_df.rename(columns={'label_ids':'label'}, inplace=True)\n","    new_df.label = new_df.label.apply(literal_eval)\n","    train_data = new_df.sample(frac=train_size, random_state=200)\n","    val_data = new_df.drop(train_data.index)\n","\n","    train_data.reset_index(drop=True, inplace=True)\n","    val_data.reset_index(drop=True, inplace=True)\n","    \n","    print(f\"FULL Dataset: {new_df.shape}\")\n","    print(f\"TRAIN Dataset: {train_data.shape}\")\n","    print(f\"VALIDATION Dataset: {val_data.shape}\")\n","\n","    return train_data, val_data\n","\n","# train_path = \"../data/MultilabelSequenceClassification/chinese_dataset/train_dataset.zip\"\n","# _train_data, _validation_data = load_dataset(train_path=train_path)"]},{"cell_type":"code","execution_count":68,"metadata":{"id":"5MbiV9ejVQto"},"outputs":[],"source":["class CustomDataset(Dataset):\n","    def __init__(self, dataframe, tokenizer, max_len):\n","        self.tokenizer = tokenizer\n","        self.data = dataframe\n","        self.text = dataframe['content']\n","        self.targets = self.data.label if \"label\" in  dataframe.columns else None\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.text)\n","\n","    def __getitem__(self, index):\n","        text = str(self.text[index])\n","        text = \" \".join(text.split())\n","\n","        inputs = self.tokenizer(\n","                                text,\n","                                None,\n","                                add_special_tokens=True,\n","                                max_length=self.max_len,\n","                                padding='max_length',\n","                                return_token_type_ids=True,\n","                                truncation=True,\n","                                return_tensors='pt'\n","                               ).to(device)\n","        inputs_single = {}\n","        inputs_single['input_ids'] = inputs['input_ids'][0]\n","        inputs_single['attention_mask'] = inputs['attention_mask'][0]\n","        inputs_single['token_type_ids'] = inputs['token_type_ids'][0]\n","        if self.targets is not None:\n","            targets = torch.tensor(self.targets[index], dtype=torch.float).to(device)\n","        else:\n","            targets = torch.tensor([]).to(device)\n","        return inputs_single, targets\n","\n","# train_dataset = CustomDataset(_train_data, tokenizer)"]},{"cell_type":"markdown","metadata":{"id":"qUDoNyk-VQtp"},"source":["### 测试模型"]},{"cell_type":"code","execution_count":69,"metadata":{"id":"XfCMMSymVQtq"},"outputs":[],"source":["def test_model(train_path, batch_size=16):\n","    train_data, validation_data = load_dataset(train_path)\n","    train_dataset = CustomDataset(train_data, tokenizer)\n","    validation_dataset = CustomDataset(validation_data, tokenizer )\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    val_loader = DataLoader(validation_dataset, batch_size=batch_size)\n","    \n","    for batch, data in enumerate(val_loader):\n","        print(batch, data)\n","        inputs, labels = data\n","        if batch == 1:\n","            break\n","    outputs = model(**inputs, labels=labels)\n","    print(outputs)\n","\n","# test_model(train_path=\"../data/MultilabelSequenceClassification/chinese_dataset/train_dataset.zip\")"]},{"cell_type":"code","execution_count":70,"metadata":{"id":"ZIKW-U2nVQtr"},"outputs":[],"source":["# break"]},{"cell_type":"markdown","metadata":{"id":"Be2WqcI9VQtr"},"source":["### 训练模型"]},{"cell_type":"code","execution_count":71,"metadata":{"id":"MPTMxhCEVQtr"},"outputs":[],"source":["# The Training Loop    \n","def train_one_epoch(model, train_loader, epoch_index, optimizer, tb_writer=None):\n","    # we’ll be using simple stochastic gradient descent with momentum\n","\n","    \n","    running_loss = 0.\n","    last_loss = 0.\n","    print(\"train_loader\", len(train_loader))\n","    # Add progress bar\n","    loop = tqdm(train_loader)\n","\n","    # Here, we use enumerate(training_loader) instead of\n","    # iter(training_loader) so that we can track the batch\n","    # index and do some intra-epoch reporting\n","    for i, data in enumerate(loop):\n","        \n","        # Every data instance is an input + label pair\n","        inputs, labels = data\n","\n","        # Zero your gradients for every batch!\n","        optimizer.zero_grad()\n","\n","        # Make predictions for this batch\n","        outputs = model(**inputs, labels=labels)\n","        # Gain loss\n","        loss = outputs.loss\n","        loss.backward()\n","\n","        # Adjust learning weights\n","        optimizer.step()\n","\n","        # Gather data \n","        running_loss += loss.item()\n","        avg_loss = running_loss / (i + 1)\n","    \n","        # Finally, it reports the average per-batch loss for the last 1000 batches, for comparison with a validation run\n","        if (tb_writer) is not None  and (i % 1000 == 999):\n","            last_loss = running_loss / 1000 # loss per batch\n","            print('batch {} loss: {}'.format(i + 1, last_loss))\n","            tb_x = epoch_index * len(train_loader) + i + 1\n","            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n","            running_loss = 0.\n","\n","        # Update progress bar\n","        loop.set_description(f\"Epoch [{epoch_index}]\") #\n","        loop.set_postfix(train_loss=avg_loss, accuracy=torch.rand(1).item()) \n","        \n","\n","    return avg_loss\n"]},{"cell_type":"code","execution_count":72,"metadata":{"id":"wP0GToTMVQts"},"outputs":[],"source":["from transformers import BertTokenizer, LongformerConfig, LongformerForSequenceClassification\n","\n","def train_model(train_path, max_len=1024, batch_size=16, epochs=10):\n","\n","    model_path = Path(\"../../models/longformer_zh\")\n","    tokenizer = BertTokenizer.from_pretrained(model_path)\n","    # config = LongformerConfig.from_pretrained(model_path)\n","    config = LongformerConfig.from_json_file(model_path /\"config.json\")\n","    # choose the attention mode 'n2', 'tvm' or 'sliding_chunks'\n","    # 'n2': for regular n2 attantion\n","    # 'tvm': a custom CUDA kernel implementation of our sliding window attention\n","    # 'sliding_chunks': a PyTorch implementation of our sliding window attention\n","    config.attention_mode = 'sliding_chunks'\n","    # If Use singe label\n","    # config.problem_type = \"single_label_classification\" If Use singe label\n","    config.problem_type = \"multi_label_classification\"\n","    config.num_labels = 65\n","    model = LongformerForSequenceClassification.from_pretrained(model_path, config=config)\n","    model = model.to(device)   \n","\n","    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n","    writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n","    best_vloss = 1_000_000.\n","\n","    train_data, validation_data = load_dataset(train_path, train_size=0.9)\n","    train_dataset = CustomDataset(train_data, tokenizer, max_len=max_len)\n","    validation_dataset = CustomDataset(validation_data, tokenizer, max_len=max_len)\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size)\n","    validation_loader = DataLoader(validation_dataset, batch_size=batch_size)\n","\n","    for name, param in model.named_parameters():\n","        if \"layer.10\" in name or \"layer.11\" in name:\n","            # print(name)\n","            param.requires_grad = True\n","        else:\n","            param.requires_grad = False\n","\n","    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5)\n","        \n","    for epoch in range(1, epochs+1):\n","        print(f'EPOCH: {epoch}')\n","        # Make sure gradient tracking is on, and do a pass over the data\n","        model.train(True)\n","        avg_loss = train_one_epoch(model, train_loader, epoch, optimizer, writer)\n","\n","        widgets = ['Validation Progress: ', Percentage(), ' ', Bar('#'),' ', Timer(),\n","                ' ', ETA(), ' ', FileTransferSpeed()]\n","        pbar = ProgressBar(widgets=widgets, maxval=len(validation_loader)).start()\n","        # Validate the model. We don't need gradients on to do reporting\n","        with torch.no_grad():\n","            running_vloss = 0.0\n","            for batch, vdata in enumerate(validation_loader):\n","                vinputs, vlabels = vdata\n","                voutputs = model(**vinputs, labels=vlabels)\n","                vloss = voutputs.loss\n","                running_vloss += vloss\n","                pbar.update(batch + 1)\n","            avg_vloss = running_vloss / (batch + 1)\n","        print(f'LOSS train {avg_loss} valid {avg_vloss}')\n","        pbar.finish()\n","        \n","        # Log the running loss averaged per batch\n","        # for both training and validation\n","        writer.add_scalars('Training vs. Validation Loss',\n","                           { 'Training' : avg_loss, 'Validation' : avg_vloss },\n","                        epoch)\n","        writer.flush()\n","\n","        # Track best performance, and save the model's state\n","        if avg_vloss < best_vloss:\n","            best_vloss = avg_vloss\n","            model_path = f'finetune_model/multi_label.pt'\n","            torch.save(model.state_dict(), model_path)\n","        torch.cuda.empty_cache()\n","    print(f\"best_vloss: {best_vloss}\")\n","\n"]},{"cell_type":"code","execution_count":73,"metadata":{"id":"I5EKDQhGVQtt"},"outputs":[{"name":"stderr","output_type":"stream","text":["The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'LongformerTokenizer'. \n","The class this function is called from is 'BertTokenizer'.\n","Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at ..\\..\\models\\longformer_zh and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["FULL Dataset: (10, 2)\n","TRAIN Dataset: (9, 2)\n","VALIDATION Dataset: (1, 2)\n","EPOCH: 1\n","train_loader 3\n"]},{"name":"stderr","output_type":"stream","text":["Epoch [1]: 100%|██████████| 3/3 [00:03<00:00,  1.31s/it, accuracy=0.39, train_loss=0.678] \n","Validation Progress: 100% |###| Elapsed Time: 0:00:00 Time: 0:00:00   3.20  B/s\n"]},{"name":"stdout","output_type":"stream","text":["LOSS train 0.6778879364331564 valid 0.65639328956604\n","EPOCH: 2\n","train_loader 3\n"]},{"name":"stderr","output_type":"stream","text":["Epoch [2]:  33%|███▎      | 1/3 [00:01<00:03,  1.71s/it, accuracy=0.593, train_loss=0.642]"]}],"source":["train_path = \"../data/MultilabelSequenceClassification/chinese_dataset/train_dataset.zip\"\n","\n","EPOCHS = 2\n","MAX_LEN = 1024\n","BATCH_SIZE = 4\n","train_model(train_path, max_len=MAX_LEN, batch_size=BATCH_SIZE, epochs=EPOCHS)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kcTlroTUVQtu"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>组织关系-裁员</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>司法行为-起诉</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>组织关系-解散</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>组织关系-加盟</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>司法行为-拘捕</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         0\n","0  组织关系-裁员\n","1  司法行为-起诉\n","2  组织关系-解散\n","3  组织关系-加盟\n","4  司法行为-拘捕"]},"execution_count":55,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","\n","df =  pd.read_csv('../data/MultilabelSequenceClassification/chinese_dataset/train.zip')\n","df.head()\n","df_label = pd.read_json('../data/MultilabelSequenceClassification/chinese_dataset/label.zip', orient='index')\n","df_label.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bEVs-6tDVQtu"},"outputs":[{"ename":"SyntaxError","evalue":"'break' outside loop (668683560.py, line 1)","output_type":"error","traceback":["\u001b[1;36m  Input \u001b[1;32mIn [56]\u001b[1;36m\u001b[0m\n\u001b[1;33m    break\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'break' outside loop\n"]}],"source":["break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"axk5RTHVVQtu"},"outputs":[],"source":["test_path = \"../data/MultilabelSequenceClassification/toxic-comment-classification/test.csv.zip\"\n","test_data = pd.read_csv(test_path)\n","test_dataset = CustomDataset(test_data, tokenizer, max_len=MAX_LEN)\n","print(f\"TEST Dataset: {test_data.shape}\")\n","test_loader = DataLoader(test_dataset, batch_size=16)  "]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["DxnX-DFLVQtZ"],"name":"longformer_suquence_classification.ipynb","version":""},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.8.8 ('py3.8')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"vscode":{"interpreter":{"hash":"c87b1f44594b67819a704c3c4238e15d055a646151ba29c648f9c587b7431b78"}}},"nbformat":4,"nbformat_minor":0}

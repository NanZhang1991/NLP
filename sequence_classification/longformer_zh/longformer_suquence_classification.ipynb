{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"background_save":true},"id":"rKzflpZ9WNtl"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/gdrive/')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"background_save":true},"id":"FrDYpPMDWbz4"},"outputs":[],"source":["# import os\n","# os.chdir('/content/gdrive/MyDrive/Colab Notebooks/sequence_classification/longformer_zh')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"background_save":true},"id":"65bkWtKFWh0u"},"outputs":[{"name":"stdout","output_type":"stream","text":[" ������ E �еľ��� m2\n"," �������к��� 6C3B-1FD3\n","\n"," e:\\NanZhang1991\\NLP\\sequence_classification\\longformer_zh ��Ŀ¼\n","\n","2022/08/18  08:07    <DIR>          .\n","2022/08/14  12:13    <DIR>          ..\n","2022/08/18  00:12    <DIR>          .ipynb_checkpoints\n","2022/07/10  22:15                 4 __init__.py\n","2022/07/10  22:15            19,059 classification.py\n","2022/08/18  02:23    <DIR>          finetune_model\n","2022/08/14  12:13    <DIR>          longformer\n","2022/08/18  02:13             9,937 longformer.ipynb\n","2022/08/18  21:34            56,635 longformer_suquence_classification.ipynb\n","2022/07/10  22:15             4,308 predict.py\n","2022/07/10  22:15             5,152 README.md\n","2022/07/10  22:15                75 requirements.txt\n","2022/08/18  21:33    <DIR>          runs\n","2022/07/18  02:41             5,157 test.ipynb\n","               8 ���ļ�        100,327 �ֽ�\n","               6 ��Ŀ¼ 206,425,665,536 �����ֽ�\n"]}],"source":["ls"]},{"cell_type":"markdown","metadata":{"id":"a_SCDw2rVQtO"},"source":["## 导入环境"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"background_save":true},"id":"kfsISrFXVQtU"},"outputs":[],"source":["# ! pip install numpy pandas torch transformers progressbar tqdm"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"05uR6kQAVQtW"},"outputs":[{"name":"stderr","output_type":"stream","text":["d:\\Program\\minicoda3\\envs\\py3.8\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["from pathlib import Path\n","from datetime import datetime\n","import pandas as pd\n","import json\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","# from torch.utils.tensorboard import SummaryWriter\n","from tqdm import tqdm\n","from progressbar import ProgressBar, Percentage, Bar, Timer, ETA, FileTransferSpeed\n","from transformers import BertTokenizer\n","from longformer.longformer import LongformerConfig, LongformerForSequenceClassification"]},{"cell_type":"markdown","metadata":{"id":"DxnX-DFLVQtZ"},"source":["## 构建数据集"]},{"cell_type":"markdown","metadata":{"id":"Tg8Qg__PVQta"},"source":["### 数据清洗"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"vFYMPKGwVQtb"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>content</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>3915</th>\n","      <td>[组织关系-退出]</td>\n","      <td>据天眼查数据显示，5月24日，上海证大喜马拉雅网络科技有限公司发生多项工商变更，小米旗下创投...</td>\n","    </tr>\n","    <tr>\n","      <th>5140</th>\n","      <td>[财经/交易-涨停]</td>\n","      <td>兴业矿业9月4日盘中涨停</td>\n","    </tr>\n","    <tr>\n","      <th>3534</th>\n","      <td>[司法行为-拘捕]</td>\n","      <td>双辽一男子酒驾被抓心怀怨恨“快手”评论区辱骂交警被拘留</td>\n","    </tr>\n","    <tr>\n","      <th>8082</th>\n","      <td>[竞赛行为-禁赛]</td>\n","      <td>昨天，根据英足总官方发布的最新消息，利物浦小将埃利奥特因为在2019赛季欧冠决赛后嘲讽热刺前...</td>\n","    </tr>\n","    <tr>\n","      <th>9915</th>\n","      <td>[竞赛行为-胜负]</td>\n","      <td>10月1日，中网ATP男子双打第一轮，中国选手公茂鑫、张哲组合在先失一盘的情况下，两下两盘，...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           label                                            content\n","3915   [组织关系-退出]  据天眼查数据显示，5月24日，上海证大喜马拉雅网络科技有限公司发生多项工商变更，小米旗下创投...\n","5140  [财经/交易-涨停]                                       兴业矿业9月4日盘中涨停\n","3534   [司法行为-拘捕]                        双辽一男子酒驾被抓心怀怨恨“快手”评论区辱骂交警被拘留\n","8082   [竞赛行为-禁赛]  昨天，根据英足总官方发布的最新消息，利物浦小将埃利奥特因为在2019赛季欧冠决赛后嘲讽热刺前...\n","9915   [竞赛行为-胜负]  10月1日，中网ATP男子双打第一轮，中国选手公茂鑫、张哲组合在先失一盘的情况下，两下两盘，..."]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["fp = '../data/MultilabelSequenceClassification/chinese_dataset/train.zip'\n","train_df = pd.read_csv(fp).fillna(value='')\n","train_df.label = train_df.label.str.split('|')\n","train_df.sample(5)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>content</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>649</th>\n","      <td>[财经/交易-涨停]</td>\n","      <td>（10-10）涨停揭秘：基础化工板块走强震安科技涨停</td>\n","    </tr>\n","    <tr>\n","      <th>944</th>\n","      <td>[人生-死亡]</td>\n","      <td>陈乔恩想必大家都不陌生，在13年版的《笑傲江湖》饰演东方不败一角，凭借甜美的外表内在的淑女气...</td>\n","    </tr>\n","    <tr>\n","      <th>677</th>\n","      <td>[司法行为-罚款]</td>\n","      <td>英国信息监管局日前发表声明称，知名连锁酒店公司万豪国际集团2018年发生客户数据泄露事件，违...</td>\n","    </tr>\n","    <tr>\n","      <th>787</th>\n","      <td>[组织关系-辞/离职]</td>\n","      <td>中国国旅总经理彭辉辞职2018年薪酬为587万元</td>\n","    </tr>\n","    <tr>\n","      <th>358</th>\n","      <td>[产品行为-上映]</td>\n","      <td>科幻电影《上海堡垒》曝海报定档8月9日</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           label                                            content\n","649   [财经/交易-涨停]                         （10-10）涨停揭秘：基础化工板块走强震安科技涨停\n","944      [人生-死亡]  陈乔恩想必大家都不陌生，在13年版的《笑傲江湖》饰演东方不败一角，凭借甜美的外表内在的淑女气...\n","677    [司法行为-罚款]  英国信息监管局日前发表声明称，知名连锁酒店公司万豪国际集团2018年发生客户数据泄露事件，违...\n","787  [组织关系-辞/离职]                           中国国旅总经理彭辉辞职2018年薪酬为587万元\n","358    [产品行为-上映]                                科幻电影《上海堡垒》曝海报定档8月9日"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["fp = '../data/MultilabelSequenceClassification/chinese_dataset/test.zip'\n","test_df = pd.read_csv(fp).fillna(value='')\n","test_df.label = test_df.label.str.split('|')\n","test_df.sample(5)"]},{"cell_type":"markdown","metadata":{"id":"ATLiGt_CVQtc"},"source":["### 标签id"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"0bUQqWP1VQtd"},"outputs":[{"data":{"text/plain":["65"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["\n","def id2label(label):\n","    labels = label.explode().drop_duplicates()\n","    id2label = dict(zip(range(len(labels)), labels))\n","    label2id = {v: k for k, v in id2label.items()}\n","\n","    with open(\"../data/MultilabelSequenceClassification/chinese_dataset/label.json\", \"w\", encoding=\"utf-8\") as f:\n","        f.write(json.dumps(id2label, ensure_ascii=False, indent=2))\n","    return id2label, label2id\n","    \n","id2label, label2id = id2label(train_df.label)\n","len(id2label)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"S4AWIjOZVQtf"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>content</th>\n","      <th>label_ids</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[组织关系-裁员]</td>\n","      <td>消失的“外企光环”，5月份在华裁员900余人，香饽饽变“臭”了</td>\n","      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[组织关系-裁员]</td>\n","      <td>前两天，被称为“仅次于苹果的软件服务商”的Oracle（甲骨文）公司突然宣布在中国裁员。。</td>\n","      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[组织关系-裁员]</td>\n","      <td>不仅仅是中国IT企业在裁员，为何500强的甲骨文也发生了全球裁员</td>\n","      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>[组织关系-加盟, 组织关系-裁员]</td>\n","      <td>据猛龙随队记者JoshLewenberg报道，消息人士透露，猛龙已将前锋萨加巴-科纳特裁掉。...</td>\n","      <td>[1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>[组织关系-裁员]</td>\n","      <td>冠军射手被裁掉，欲加入湖人队，但湖人却无意，冠军射手何去何从</td>\n","      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                label                                            content  \\\n","0           [组织关系-裁员]                    消失的“外企光环”，5月份在华裁员900余人，香饽饽变“臭”了   \n","1           [组织关系-裁员]      前两天，被称为“仅次于苹果的软件服务商”的Oracle（甲骨文）公司突然宣布在中国裁员。。   \n","2           [组织关系-裁员]                   不仅仅是中国IT企业在裁员，为何500强的甲骨文也发生了全球裁员   \n","3  [组织关系-加盟, 组织关系-裁员]  据猛龙随队记者JoshLewenberg报道，消息人士透露，猛龙已将前锋萨加巴-科纳特裁掉。...   \n","4           [组织关系-裁员]                     冠军射手被裁掉，欲加入湖人队，但湖人却无意，冠军射手何去何从   \n","\n","                                           label_ids  \n","0  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n","1  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n","2  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n","3  [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n","4  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["def label_ids(label):\n","    label_ids = [0] * len(id2label)\n","    for separate_label in label:\n","        label_id = label2id.get(separate_label)\n","        if label_id is not None:\n","            label_ids[label_id] = 1\n","    return  label_ids\n","\n","train_df['label_ids'] = train_df.label.apply(label_ids)\n","train_df.to_csv('../data/MultilabelSequenceClassification/chinese_dataset/train_dataset.zip')\n","train_df.head()\n","\n","test_df['label_ids'] = test_df.label.apply(label_ids)\n","test_df.to_csv('../data/MultilabelSequenceClassification/chinese_dataset/test_dataset.zip')\n","test_df.head()"]},{"cell_type":"markdown","metadata":{"id":"dil0GxRMVQti"},"source":["## 模型训练"]},{"cell_type":"markdown","metadata":{"id":"zz4OWMBwVQtj"},"source":["### 加载预训练模型"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"bIGyer1vVQtk"},"outputs":[{"data":{"text/plain":["device(type='cpu')"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device = torch.device('cpu')\n","device"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"JQu0ihB_VQtl"},"outputs":[],"source":["# model_path = Path(\"../../models/distilbert-base-uncased\")\n","# tokenizer = BertTokenizer.from_pretrained(model_path)\n","# config = LongformerConfig.from_pretrained(model_path)\n","# config.problem_type = \"multi_label_classification\"\n","# config.num_labels = 6\n","# model = BertForSequenceClassification.from_pretrained(model_path, config=config)\n","# model.to(device)"]},{"cell_type":"markdown","metadata":{"id":"yiEC3ZDdVQtn"},"source":["### 加载数据集"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"yGVmBfEaVQtn"},"outputs":[],"source":["import pandas as pd \n","from ast import literal_eval\n","\n","def load_dataset(train_path, train_size=0.9):\n","    df = pd.read_csv(train_path)\n","    new_df = df[['content', 'label_ids']].copy()\n","    new_df.rename(columns={'label_ids':'label'}, inplace=True)\n","    new_df.label = new_df.label.apply(literal_eval)\n","    train_data = new_df.sample(frac=train_size, random_state=200)\n","    val_data = new_df.drop(train_data.index)\n","\n","    train_data.reset_index(drop=True, inplace=True)\n","    val_data.reset_index(drop=True, inplace=True)\n","    \n","    print(f\"FULL Dataset: {new_df.shape}\")\n","    print(f\"TRAIN Dataset: {train_data.shape}\")\n","    print(f\"VALIDATION Dataset: {val_data.shape}\")\n","\n","    return train_data, val_data\n","\n","# train_path = \"../data/MultilabelSequenceClassification/chinese_dataset/train_dataset.zip\"\n","# _train_data, _validation_data = load_dataset(train_path=train_path)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"5MbiV9ejVQto"},"outputs":[],"source":["class CustomDataset(Dataset):\n","    def __init__(self, dataframe, tokenizer, max_len):\n","        self.tokenizer = tokenizer\n","        self.data = dataframe\n","        self.text = dataframe['content']\n","        self.targets = self.data.label if \"label\" in  dataframe.columns else None\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.text)\n","\n","    def __getitem__(self, index):\n","        text = str(self.text[index])\n","        text = \" \".join(text.split())\n","\n","        inputs = self.tokenizer(\n","                                text,\n","                                None,\n","                                add_special_tokens=True,\n","                                max_length=self.max_len,\n","                                padding='max_length',\n","                                return_token_type_ids=True,\n","                                truncation=True,\n","                                return_tensors='pt'\n","                               ).to(device)\n","        inputs_single = {}\n","        inputs_single['input_ids'] = inputs['input_ids'][0]\n","        inputs_single['attention_mask'] = inputs['attention_mask'][0]\n","        inputs_single['token_type_ids'] = inputs['token_type_ids'][0]\n","        if self.targets is not None:\n","            targets = torch.tensor(self.targets[index], dtype=torch.float).to(device)\n","        else:\n","            targets = torch.tensor([]).to(device)\n","        return inputs_single, targets\n","\n","# train_dataset = CustomDataset(_train_data, tokenizer)"]},{"cell_type":"markdown","metadata":{"id":"qUDoNyk-VQtp"},"source":["### 测试模型"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"XfCMMSymVQtq"},"outputs":[],"source":["def test_model(train_path, batch_size=16):\n","    train_data, validation_data = load_dataset(train_path)\n","    train_dataset = CustomDataset(train_data, tokenizer)\n","    validation_dataset = CustomDataset(validation_data, tokenizer )\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    val_loader = DataLoader(validation_dataset, batch_size=batch_size)\n","    \n","    for batch, data in enumerate(val_loader):\n","        print(batch, data)\n","        inputs, labels = data\n","        if batch == 1:\n","            break\n","    outputs = model(**inputs, labels=labels)\n","    print(outputs)\n","\n","# test_model(train_path=\"../data/MultilabelSequenceClassification/chinese_dataset/train_dataset.zip\")"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"ZIKW-U2nVQtr"},"outputs":[],"source":["# break"]},{"cell_type":"markdown","metadata":{"id":"Be2WqcI9VQtr"},"source":["### 训练模型"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"MPTMxhCEVQtr"},"outputs":[],"source":["# The Training Loop    \n","def train_one_epoch(model, train_loader, epoch_index, optimizer, tb_writer=None):\n","    # we’ll be using simple stochastic gradient descent with momentum\n","\n","    \n","    running_loss = 0.\n","    last_loss = 0.\n","    # Add progress bar\n","    loop = tqdm(train_loader)\n","\n","    # Here, we use enumerate(training_loader) instead of\n","    # iter(training_loader) so that we can track the batch\n","    # index and do some intra-epoch reporting\n","    for i, data in enumerate(loop):\n","        \n","        # Every data instance is an input + label pair\n","        inputs, labels = data\n","\n","        # Zero your gradients for every batch!\n","        optimizer.zero_grad()\n","\n","        # Make predictions for this batch\n","        outputs = model(**inputs, labels=labels)\n","        # Gain loss\n","        loss = outputs.loss\n","        loss.backward()\n","\n","        # Adjust learning weights\n","        optimizer.step()\n","\n","        # Gather data \n","        running_loss += loss.item()\n","        avg_loss = running_loss / (i + 1)\n","    \n","        # Finally, it reports the average per-batch loss for the last 1000 batches, for comparison with a validation run\n","        if (tb_writer) is not None  and (i % 1000 == 999):\n","            last_loss = running_loss / 1000 # loss per batch\n","            print('batch {} loss: {}'.format(i + 1, last_loss))\n","            tb_x = epoch_index * len(train_loader) + i + 1\n","            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n","            running_loss = 0.\n","\n","        # Update progress bar\n","        loop.set_description(f\"Epoch [{epoch_index}]\") #\n","        loop.set_postfix(train_loss=avg_loss) \n","        \n","\n","    return avg_loss\n"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"wP0GToTMVQts"},"outputs":[],"source":["# from transformers import BertTokenizer, LongformerConfig, LongformerForSequenceClassification\n","\n","def train_model(train_path, max_len=1024, batch_size=16, epochs=10):\n","\n","    model_path = Path(\"../../models/longformer_zh\")\n","    tokenizer = BertTokenizer.from_pretrained(model_path)\n","    # config = LongformerConfig.from_pretrained(model_path)\n","    config = LongformerConfig.from_json_file(model_path /\"config.json\")\n","    # choose the attention mode 'n2', 'tvm' or 'sliding_chunks'\n","    # 'n2': for regular n2 attantion\n","    # 'tvm': a custom CUDA kernel implementation of our sliding window attention\n","    # 'sliding_chunks': a PyTorch implementation of our sliding window attention\n","    config.attention_mode = 'sliding_chunks'\n","    # If Use singe label\n","    # config.problem_type = \"single_label_classification\" If Use singe label\n","    config.problem_type = \"multi_label_classification\"\n","    config.num_labels = 65\n","    model = LongformerForSequenceClassification.from_pretrained(model_path, config=config)\n","    model = model.to(device)   \n","\n","    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n","    writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n","    best_vloss = 1_000_000.\n","\n","    train_data, validation_data = load_dataset(train_path, train_size=0.9)\n","    train_dataset = CustomDataset(train_data, tokenizer, max_len=max_len)\n","    validation_dataset = CustomDataset(validation_data, tokenizer, max_len=max_len)\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size)\n","    print(f\"train_loader size {len(train_loader)}\")\n","    validation_loader = DataLoader(validation_dataset, batch_size=batch_size)\n","\n","    # for name, param in model.named_parameters():\n","    #     if \"layer.10\" in name or \"layer.11\" in name:\n","    #         # print(name)\n","    #         param.requires_grad = True\n","    #     else:\n","    #         param.requires_grad = False\n","\n","    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5)\n","        \n","    for epoch in range(1, epochs+1):\n","        print(f'EPOCH: {epoch}')\n","        # Make sure gradient tracking is on, and do a pass over the data\n","        model.train(True)\n","        avg_loss = train_one_epoch(model, train_loader, epoch, optimizer, writer)\n","\n","        widgets = ['Validation Progress: ', Percentage(), ' ', Bar('#'),' ', Timer(),\n","                ' ', ETA(), ' ', FileTransferSpeed()]\n","        pbar = ProgressBar(widgets=widgets, maxval=len(validation_loader)).start()\n","        # Validate the model. We don't need gradients on to do reporting\n","        with torch.no_grad():\n","            running_vloss = 0.0\n","            for batch, vdata in enumerate(validation_loader):\n","                vinputs, vlabels = vdata\n","                voutputs = model(**vinputs, labels=vlabels)\n","                vloss = voutputs.loss\n","                running_vloss += vloss\n","                pbar.update(batch + 1)\n","            avg_vloss = running_vloss / (batch + 1)\n","        print(f'LOSS train {avg_loss} valid {avg_vloss}')\n","        pbar.finish()\n","        \n","        # Log the running loss averaged per batch\n","        # for both training and validation\n","        writer.add_scalars('Training vs. Validation Loss',\n","                           { 'Training' : avg_loss, 'Validation' : avg_vloss },\n","                        epoch)\n","        writer.flush()\n","\n","        # Track best performance, and save the model's state\n","        if avg_vloss < best_vloss:\n","            best_vloss = avg_vloss\n","            model_path = f'finetune_model/multi_label.pt'\n","            torch.save(model.state_dict(), model_path)\n","        torch.cuda.empty_cache()\n","    print(f\"best_vloss: {best_vloss}\")\n","\n"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"I5EKDQhGVQtt"},"outputs":[{"name":"stderr","output_type":"stream","text":["The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'LongformerTokenizer'. \n","The class this function is called from is 'BertTokenizer'.\n"]},{"name":"stdout","output_type":"stream","text":["----------------- LongformerConfig {\n","  \"_name_or_path\": \"..\\\\..\\\\models\\\\longformer_zh\",\n","  \"architectures\": [\n","    \"LongformerModel\"\n","  ],\n","  \"attention_dilation\": null,\n","  \"attention_mode\": \"sliding_chunks\",\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"attention_window\": [\n","    512,\n","    512,\n","    512,\n","    512,\n","    512,\n","    512,\n","    512,\n","    512,\n","    512,\n","    512,\n","    512,\n","    512\n","  ],\n","  \"autoregressive\": false,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"directionality\": \"bidi\",\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\",\n","    \"6\": \"LABEL_6\",\n","    \"7\": \"LABEL_7\",\n","    \"8\": \"LABEL_8\",\n","    \"9\": \"LABEL_9\",\n","    \"10\": \"LABEL_10\",\n","    \"11\": \"LABEL_11\",\n","    \"12\": \"LABEL_12\",\n","    \"13\": \"LABEL_13\",\n","    \"14\": \"LABEL_14\",\n","    \"15\": \"LABEL_15\",\n","    \"16\": \"LABEL_16\",\n","    \"17\": \"LABEL_17\",\n","    \"18\": \"LABEL_18\",\n","    \"19\": \"LABEL_19\",\n","    \"20\": \"LABEL_20\",\n","    \"21\": \"LABEL_21\",\n","    \"22\": \"LABEL_22\",\n","    \"23\": \"LABEL_23\",\n","    \"24\": \"LABEL_24\",\n","    \"25\": \"LABEL_25\",\n","    \"26\": \"LABEL_26\",\n","    \"27\": \"LABEL_27\",\n","    \"28\": \"LABEL_28\",\n","    \"29\": \"LABEL_29\",\n","    \"30\": \"LABEL_30\",\n","    \"31\": \"LABEL_31\",\n","    \"32\": \"LABEL_32\",\n","    \"33\": \"LABEL_33\",\n","    \"34\": \"LABEL_34\",\n","    \"35\": \"LABEL_35\",\n","    \"36\": \"LABEL_36\",\n","    \"37\": \"LABEL_37\",\n","    \"38\": \"LABEL_38\",\n","    \"39\": \"LABEL_39\",\n","    \"40\": \"LABEL_40\",\n","    \"41\": \"LABEL_41\",\n","    \"42\": \"LABEL_42\",\n","    \"43\": \"LABEL_43\",\n","    \"44\": \"LABEL_44\",\n","    \"45\": \"LABEL_45\",\n","    \"46\": \"LABEL_46\",\n","    \"47\": \"LABEL_47\",\n","    \"48\": \"LABEL_48\",\n","    \"49\": \"LABEL_49\",\n","    \"50\": \"LABEL_50\",\n","    \"51\": \"LABEL_51\",\n","    \"52\": \"LABEL_52\",\n","    \"53\": \"LABEL_53\",\n","    \"54\": \"LABEL_54\",\n","    \"55\": \"LABEL_55\",\n","    \"56\": \"LABEL_56\",\n","    \"57\": \"LABEL_57\",\n","    \"58\": \"LABEL_58\",\n","    \"59\": \"LABEL_59\",\n","    \"60\": \"LABEL_60\",\n","    \"61\": \"LABEL_61\",\n","    \"62\": \"LABEL_62\",\n","    \"63\": \"LABEL_63\",\n","    \"64\": \"LABEL_64\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_10\": 10,\n","    \"LABEL_11\": 11,\n","    \"LABEL_12\": 12,\n","    \"LABEL_13\": 13,\n","    \"LABEL_14\": 14,\n","    \"LABEL_15\": 15,\n","    \"LABEL_16\": 16,\n","    \"LABEL_17\": 17,\n","    \"LABEL_18\": 18,\n","    \"LABEL_19\": 19,\n","    \"LABEL_2\": 2,\n","    \"LABEL_20\": 20,\n","    \"LABEL_21\": 21,\n","    \"LABEL_22\": 22,\n","    \"LABEL_23\": 23,\n","    \"LABEL_24\": 24,\n","    \"LABEL_25\": 25,\n","    \"LABEL_26\": 26,\n","    \"LABEL_27\": 27,\n","    \"LABEL_28\": 28,\n","    \"LABEL_29\": 29,\n","    \"LABEL_3\": 3,\n","    \"LABEL_30\": 30,\n","    \"LABEL_31\": 31,\n","    \"LABEL_32\": 32,\n","    \"LABEL_33\": 33,\n","    \"LABEL_34\": 34,\n","    \"LABEL_35\": 35,\n","    \"LABEL_36\": 36,\n","    \"LABEL_37\": 37,\n","    \"LABEL_38\": 38,\n","    \"LABEL_39\": 39,\n","    \"LABEL_4\": 4,\n","    \"LABEL_40\": 40,\n","    \"LABEL_41\": 41,\n","    \"LABEL_42\": 42,\n","    \"LABEL_43\": 43,\n","    \"LABEL_44\": 44,\n","    \"LABEL_45\": 45,\n","    \"LABEL_46\": 46,\n","    \"LABEL_47\": 47,\n","    \"LABEL_48\": 48,\n","    \"LABEL_49\": 49,\n","    \"LABEL_5\": 5,\n","    \"LABEL_50\": 50,\n","    \"LABEL_51\": 51,\n","    \"LABEL_52\": 52,\n","    \"LABEL_53\": 53,\n","    \"LABEL_54\": 54,\n","    \"LABEL_55\": 55,\n","    \"LABEL_56\": 56,\n","    \"LABEL_57\": 57,\n","    \"LABEL_58\": 58,\n","    \"LABEL_59\": 59,\n","    \"LABEL_6\": 6,\n","    \"LABEL_60\": 60,\n","    \"LABEL_61\": 61,\n","    \"LABEL_62\": 62,\n","    \"LABEL_63\": 63,\n","    \"LABEL_64\": 64,\n","    \"LABEL_7\": 7,\n","    \"LABEL_8\": 8,\n","    \"LABEL_9\": 9\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 4096,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_fc_size\": 768,\n","  \"pooler_num_attention_heads\": 12,\n","  \"pooler_num_fc_layers\": 3,\n","  \"pooler_size_per_head\": 128,\n","  \"pooler_type\": \"first_token_transform\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"multi_label_classification\",\n","  \"sep_token_id\": 2,\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 21128\n","}\n","\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at ..\\..\\models\\longformer_zh and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["FULL Dataset: (11958, 2)\n","TRAIN Dataset: (10762, 2)\n","VALIDATION Dataset: (1196, 2)\n","train_loader size 1794\n","EPOCH: 1\n"]},{"name":"stderr","output_type":"stream","text":["Epoch [1]:   1%|▏         | 26/1794 [34:19<39:16:52, 79.98s/it, train_loss=0.504]"]}],"source":["train_path = \"../data/MultilabelSequenceClassification/chinese_dataset/train_dataset.zip\"\n","\n","EPOCHS = 5\n","MAX_LEN = 1024\n","BATCH_SIZE = 6\n","train_model(train_path, max_len=MAX_LEN, batch_size=BATCH_SIZE, epochs=EPOCHS)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kcTlroTUVQtu"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>组织关系-裁员</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>司法行为-起诉</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>组织关系-解散</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>组织关系-加盟</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>司法行为-拘捕</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         0\n","0  组织关系-裁员\n","1  司法行为-起诉\n","2  组织关系-解散\n","3  组织关系-加盟\n","4  司法行为-拘捕"]},"execution_count":74,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","\n","df =  pd.read_csv('../data/MultilabelSequenceClassification/chinese_dataset/train.zip')\n","df.head()\n","df_label = pd.read_json('../data/MultilabelSequenceClassification/chinese_dataset/label.zip', orient='index')\n","df_label.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bEVs-6tDVQtu"},"outputs":[{"ename":"SyntaxError","evalue":"'break' outside loop (668683560.py, line 1)","output_type":"error","traceback":["\u001b[1;36m  Input \u001b[1;32mIn [75]\u001b[1;36m\u001b[0m\n\u001b[1;33m    break\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'break' outside loop\n"]}],"source":["break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"axk5RTHVVQtu"},"outputs":[],"source":["test_path = \"../data/MultilabelSequenceClassification/toxic-comment-classification/test.csv.zip\"\n","test_data = pd.read_csv(test_path)\n","test_dataset = CustomDataset(test_data, tokenizer, max_len=MAX_LEN)\n","print(f\"TEST Dataset: {test_data.shape}\")\n","test_loader = DataLoader(test_dataset, batch_size=16)  "]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["DxnX-DFLVQtZ"],"name":"longformer_suquence_classification.ipynb","version":""},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.8.8 ('py3.8')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"vscode":{"interpreter":{"hash":"c87b1f44594b67819a704c3c4238e15d055a646151ba29c648f9c587b7431b78"}}},"nbformat":4,"nbformat_minor":0}

{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"rKzflpZ9WNtl"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive/')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"FrDYpPMDWbz4"},"outputs":[],"source":["import os\n","os.chdir('/content/gdrive/MyDrive/Colab Notebooks/sequence_classification/longformer_zh')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"65bkWtKFWh0u"},"outputs":[{"name":"stdout","output_type":"stream","text":["classification.py  longformer_suquence_classification.ipynb  \u001b[0m\u001b[01;34mruns\u001b[0m/\n","__init__.py        predict.py                                test.ipynb\n","\u001b[01;34mlongformer\u001b[0m/        README.md\n","longformer.ipynb   requirements.txt\n"]}],"source":["ls"]},{"cell_type":"markdown","metadata":{"id":"a_SCDw2rVQtO"},"source":["## 导入环境"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"kfsISrFXVQtU"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.0+cu113)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.20.1)\n","Requirement already satisfied: progressbar in /usr/local/lib/python3.7/dist-packages (2.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.8.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n"]}],"source":["! pip install numpy pandas torch transformers progressbar tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"05uR6kQAVQtW"},"outputs":[],"source":["from pathlib import Path\n","from datetime import datetime\n","import pandas as pd\n","import json\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","from tqdm import tqdm\n","from progressbar import ProgressBar, Percentage, Bar, Timer, ETA, FileTransferSpeed\n","from transformers import BertTokenizer\n","from longformer.longformer import LongformerConfig, LongformerForSequenceClassification"]},{"cell_type":"markdown","metadata":{"id":"DxnX-DFLVQtZ"},"source":["## 构建数据集"]},{"cell_type":"markdown","metadata":{"id":"Tg8Qg__PVQta"},"source":["### 数据清洗"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vFYMPKGwVQtb"},"outputs":[],"source":["fp = '../data/MultilabelSequenceClassification/chinese_dataset/train.zip'\n","train_df = pd.read_csv(fp).fillna(value='')\n","train_df.label = train_df.label.str.split('|')\n","train_df.sample(5)"]},{"cell_type":"markdown","metadata":{"id":"ATLiGt_CVQtc"},"source":["### 标签id"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0bUQqWP1VQtd"},"outputs":[],"source":["\n","def id2label(label):\n","    labels = label.explode().drop_duplicates()\n","    id2label = dict(zip(range(len(labels)), labels))\n","    label2id = {v: k for k, v in id2label.items()}\n","\n","    with open(\"../data/MultilabelSequenceClassification/chinese_dataset/label.json\", \"w\", encoding=\"utf-8\") as f:\n","        f.write(json.dumps(id2label, ensure_ascii=False, indent=2))\n","    return id2label, label2id\n","    \n","id2label, label2id = id2label(train_df.label)\n","len(id2label)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S4AWIjOZVQtf"},"outputs":[],"source":["def label_ids(label):\n","    label_ids = [0] * len(id2label)\n","    for separate_label in label:\n","        label_id = label2id.get(separate_label)\n","        if label_id is not None:\n","            label_ids[label_id] = 1\n","    return  label_ids\n","\n","train_df['label_ids'] = train_df.label.apply(label_ids)\n","train_df.to_csv('../data/MultilabelSequenceClassification/chinese_dataset/train_dataset.zip')\n","train_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5G12DapnVQth"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"dil0GxRMVQti"},"source":["## 模型训练"]},{"cell_type":"markdown","metadata":{"id":"zz4OWMBwVQtj"},"source":["### 加载预训练模型"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bIGyer1vVQtk"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# device = torch.device('cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JQu0ihB_VQtl"},"outputs":[],"source":["# model_path = Path(\"../../models/distilbert-base-uncased\")\n","# tokenizer = BertTokenizer.from_pretrained(model_path)\n","# config = LongformerConfig.from_pretrained(model_path)\n","# config.problem_type = \"multi_label_classification\"\n","# config.num_labels = 6\n","# model = BertForSequenceClassification.from_pretrained(model_path, config=config)\n","# model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9bGOB4-AVQtm"},"outputs":[],"source":["model_path = Path(\"../../models/longformer_zh\")\n","tokenizer = BertTokenizer.from_pretrained(model_path)\n","# config = LongformerConfig.from_pretrained(model_path)\n","config = LongformerConfig.from_json_file(model_path /\"config.json\")\n","# choose the attention mode 'n2', 'tvm' or 'sliding_chunks'\n","# 'n2': for regular n2 attantion\n","# 'tvm': a custom CUDA kernel implementation of our sliding window attention\n","# 'sliding_chunks': a PyTorch implementation of our sliding window attention\n","config.attention_mode = 'sliding_chunks'\n","# If Use singe label\n","# config.problem_type = \"single_label_classification\" If Use singe label\n","config.problem_type = \"multi_label_classification\"\n","config.num_labels = 65\n","model = LongformerForSequenceClassification.from_pretrained(model_path, config=config)\n","model.to(device)"]},{"cell_type":"markdown","metadata":{"id":"yiEC3ZDdVQtn"},"source":["### 加载数据集"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yGVmBfEaVQtn"},"outputs":[],"source":["import pandas as pd \n","from ast import literal_eval\n","\n","def load_dataset(train_path, train_size=0.9):\n","    df = pd.read_csv(train_path)[:1000]\n","    new_df = df[['content', 'label_ids']].copy()\n","    new_df.rename(columns={'label_ids':'label'}, inplace=True)\n","    new_df.label = new_df.label.apply(literal_eval)\n","    print(f'df: {new_df.head()}')\n","    train_data = new_df.sample(frac=train_size, random_state=200)\n","    val_data = new_df.drop(train_data.index)\n","\n","\n","    train_data.reset_index(drop=True, inplace=True)\n","    val_data.reset_index(drop=True, inplace=True)\n","    \n","    print(f\"FULL Dataset: {new_df.shape}\")\n","    print(f\"TRAIN Dataset: {train_data.shape}\")\n","    print(f\"VALIDATION Dataset: {val_data.shape}\")\n","\n","    return train_data, val_data\n","\n","# train_path = \"../data/MultilabelSequenceClassification/chinese_dataset/train_dataset.zip\"\n","# _train_data, _validation_data = load_dataset(train_path=train_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5MbiV9ejVQto"},"outputs":[],"source":["class CustomDataset(Dataset):\n","    def __init__(self, dataframe, tokenizer, max_len=config.max_position_embeddings):\n","        self.tokenizer = tokenizer\n","        self.data = dataframe\n","        self.text = dataframe['content']\n","        self.targets = self.data.label if \"label\" in  dataframe.columns else None\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.text)\n","\n","    def __getitem__(self, index):\n","        text = str(self.text[index])\n","        text = \" \".join(text.split())\n","\n","        inputs = self.tokenizer(\n","                                text,\n","                                None,\n","                                add_special_tokens=True,\n","                                max_length=self.max_len,\n","                                padding='max_length',\n","                                return_token_type_ids=True,\n","                                truncation=True,\n","                                return_tensors='pt'\n","                               ).to(device)\n","        inputs_single = {}\n","        inputs_single['input_ids'] = inputs['input_ids'][0]\n","        inputs_single['attention_mask'] = inputs['attention_mask'][0]\n","        inputs_single['token_type_ids'] = inputs['token_type_ids'][0]\n","        if self.targets is not None:\n","            targets = torch.tensor(self.targets[index], dtype=torch.float).to(device)\n","        else:\n","            targets = torch.tensor([])\n","        return inputs_single, targets\n","\n","# train_dataset = CustomDataset(_train_data, tokenizer)"]},{"cell_type":"markdown","metadata":{"id":"qUDoNyk-VQtp"},"source":["### 测试模型"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XfCMMSymVQtq"},"outputs":[],"source":["def test_model(train_path, batch_size=16):\n","    train_data, validation_data = load_dataset(train_path)\n","    train_dataset = CustomDataset(train_data, tokenizer)\n","    validation_dataset = CustomDataset(validation_data, tokenizer )\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    val_loader = DataLoader(validation_dataset, batch_size=batch_size)\n","    \n","    for batch, data in enumerate(val_loader):\n","        print(batch, data)\n","        inputs, labels = data\n","        if batch == 1:\n","            break\n","    outputs = model(**inputs, labels=labels)\n","    print(outputs)\n","\n","# test_model(train_path=\"../data/MultilabelSequenceClassification/chinese_dataset/train_dataset.zip\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZIKW-U2nVQtr"},"outputs":[],"source":["# break"]},{"cell_type":"markdown","metadata":{"id":"Be2WqcI9VQtr"},"source":["### 训练模型"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MPTMxhCEVQtr"},"outputs":[],"source":["# The Training Loop    \n","def train_one_epoch(train_loader, epoch_index, tb_writer=None):\n","    # we’ll be using simple stochastic gradient descent with momentum\n","\n","    \n","\n","    optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-5)\n","    running_loss = 0.\n","    last_loss = 0.\n","    print(\"train_loader\", len(train_loader))\n","    # Add progress bar\n","    loop = tqdm(train_loader)\n","\n","    # Here, we use enumerate(training_loader) instead of\n","    # iter(training_loader) so that we can track the batch\n","    # index and do some intra-epoch reporting\n","    for i, data in enumerate(loop):\n","        \n","        # Every data instance is an input + label pair\n","        inputs, labels = data\n","\n","        # Zero your gradients for every batch!\n","        optimizer.zero_grad()\n","\n","        # Make predictions for this batch\n","        outputs = model(**inputs, labels=labels)\n","        # Gain loss\n","        loss = outputs.loss\n","        loss.backward()\n","\n","        # Adjust learning weights\n","        optimizer.step()\n","\n","        # Gather data \n","        running_loss += loss.item()\n","        avg_loss = running_loss / (i + 1)\n","    \n","        # Finally, it reports the average per-batch loss for the last 1000 batches, for comparison with a validation run\n","        if (tb_writer) is not None  and (i % 1000 == 999):\n","            last_loss = running_loss / 1000 # loss per batch\n","            print('batch {} loss: {}'.format(i + 1, last_loss))\n","            tb_x = epoch_index * len(train_loader) + i + 1\n","            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n","            running_loss = 0.\n","\n","        # Update progress bar\n","        loop.set_description(f\"Epoch [{epoch_index}]\") #\n","        loop.set_postfix(train_loss=avg_loss, accuracy=torch.rand(1).item()) \n","        \n","\n","    return avg_loss\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wP0GToTMVQts"},"outputs":[],"source":["def train_model(train_path, max_len=1024, batch_size=16, epochs=10):\n","    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n","    writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n","    best_vloss = 1_000_000.\n","\n","    train_data, validation_data = load_dataset(train_path, train_size=0.9)\n","    train_dataset = CustomDataset(train_data, tokenizer, max_len=max_len)\n","    validation_dataset = CustomDataset(validation_data, tokenizer, max_len=max_len)\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size)\n","    validation_loader = DataLoader(validation_dataset, batch_size=batch_size)\n","\n","    for epoch in range(1, epochs+1):\n","        print(f'EPOCH: {epoch}')\n","        # Make sure gradient tracking is on, and do a pass over the data\n","        model.train(True)\n","        avg_loss = train_one_epoch(train_loader, epoch, writer)\n","\n","        widgets = ['Validation Progress: ', Percentage(), ' ', Bar('#'),' ', Timer(),\n","                ' ', ETA(), ' ', FileTransferSpeed()]\n","        pbar = ProgressBar(widgets=widgets, maxval=len(validation_loader)).start()\n","        # Validate the model. We don't need gradients on to do reporting\n","        with torch.no_grad():\n","            running_vloss = 0.0\n","            for batch, vdata in enumerate(validation_loader):\n","                vinputs, vlabels = vdata\n","                voutputs = model(**vinputs, labels=vlabels)\n","                vloss = voutputs.loss\n","                running_vloss += vloss\n","                pbar.update(batch + 1)\n","            avg_vloss = running_vloss / (batch + 1)\n","        print(f'LOSS train {avg_loss} valid {avg_vloss}')\n","        pbar.finish()\n","        \n","        # Log the running loss averaged per batch\n","        # for both training and validation\n","        writer.add_scalars('Training vs. Validation Loss',\n","                           { 'Training' : avg_loss, 'Validation' : avg_vloss },\n","                        epoch)\n","        writer.flush()\n","\n","        # Track best performance, and save the model's state\n","        if avg_vloss < best_vloss:\n","            best_vloss = avg_vloss\n","            model_path = f'test_model/model_{timestamp}_{best_vloss}.pt'\n","            torch.save(model.state_dict(), model_path)\n","        torch.cuda.empty_cache()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I5EKDQhGVQtt"},"outputs":[],"source":["train_path = \"../data/MultilabelSequenceClassification/chinese_dataset/train_dataset.zip\"\n","\n","EPOCHS = 2\n","MAX_LEN = 1024\n","BATCH_SIZE = 4\n","train_model(train_path, max_len=MAX_LEN, batch_size=BATCH_SIZE, epochs=EPOCHS)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kcTlroTUVQtu"},"outputs":[],"source":["import pandas as pd\n","\n","df =  pd.read_csv('../data/MultilabelSequenceClassification/chinese_dataset/train.zip')\n","df.head()\n","df_label = pd.read_json('../data/MultilabelSequenceClassification/chinese_dataset/label.zip', orient='index')\n","df_label.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bEVs-6tDVQtu"},"outputs":[],"source":["break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"axk5RTHVVQtu"},"outputs":[],"source":["test_path = \"../data/MultilabelSequenceClassification/toxic-comment-classification/test.csv.zip\"\n","test_data = pd.read_csv(test_path)\n","test_dataset = CustomDataset(test_data, tokenizer, max_len=MAX_LEN)\n","print(f\"TEST Dataset: {test_data.shape}\")\n","test_loader = DataLoader(test_dataset, batch_size=16)  "]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["DxnX-DFLVQtZ"],"name":"longformer_suquence_classification.ipynb","version":""},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.8.8 ('py3.8')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"vscode":{"interpreter":{"hash":"c87b1f44594b67819a704c3c4238e15d055a646151ba29c648f9c587b7431b78"}}},"nbformat":4,"nbformat_minor":0}

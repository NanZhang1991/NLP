{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../../models/longformer-chinese-base-4096 were not used when initializing Longformer: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing Longformer from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Longformer from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from longformer.longformer import LongformerConfig, Longformer, LongformerForSequenceClassification\n",
    "\n",
    "model_path = \"../../models/longformer-chinese-base-4096\"\n",
    "config = LongformerConfig.from_pretrained(model_path)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "model = Longformer.from_pretrained(model_path, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from longformer.longformer import LongformerConfig, LongformerForSequenceClassification\n",
    "\n",
    "model_path = \"../../models/longformer_zh\"\n",
    "config = LongformerConfig.from_pretrained(model_path)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "model = LongformerForSequenceClassification.from_pretrained(model_path, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type longformer to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LongformerTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from longformer.longformer import LongformerConfig, Longformer\n",
    "\n",
    "model_path = \"../../models/longformer_zh\"\n",
    "config = LongformerConfig.from_pretrained(model_path)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "model = Longformer.from_pretrained(model_path, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LongformerTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, BertTokenizer, BertModel, BertConfig, LongformerModel, LongformerConfig\n",
    "# model_path = \"../../models/longformer-chinese-base-4096\"\n",
    "model_path = \"../../models/longformer_zh\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "config = LongformerConfig.from_pretrained(model_path)\n",
    "model = AutoModel.from_pretrained(model_path, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LongformerTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at ../../models/longformer_zh and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, LongformerConfig, LongformerForSequenceClassification\n",
    "model_path = \"../../models/longformer_zh\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "model = LongformerForSequenceClassification.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LongformerTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at ../../models/longformer_zh and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"../../models/longformer_zh\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"../../models/longformer_zh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0*   Trying 142.251.42.227:80...\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:02 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:03 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:04 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:05 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:06 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:07 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:08 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:09 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:10 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:11 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:12 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:13 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:14 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:15 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:16 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:17 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:18 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:19 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:20 --:--:--     0* connect to 142.251.42.227 port 80 failed: Timed out\n",
      "* Failed to connect to www.googel.com port 80 after 21061 ms: Timed out\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:21 --:--:--     0\n",
      "* Closing connection 0\n",
      "curl: (28) Failed to connect to www.googel.com port 80 after 21061 ms: Timed out\n"
     ]
    }
   ],
   "source": [
    "!curl -v www.googel.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c87b1f44594b67819a704c3c4238e15d055a646151ba29c648f9c587b7431b78"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

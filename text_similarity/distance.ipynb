{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltp import LTP\n",
    "ltp = LTP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sents:['2017年11月21日，全国中小企业股份转让系统有限责任公司出具《关于同意江苏鸿基节能新技术股份有限公司终止股票全国中小企业股份转让系统挂牌的函》“股转系统函 [2017]6631号”，        同意鸿基节能股票自2017年11月24日在全国中小企业股份转让系统终止挂牌。', '同日，鸿基节能出具《关于江苏鸿基节能新技术股份有限公司股票终止挂牌的公告》，        鸿基节能决定自2017年11月24日起终止其股票挂牌'], segments:[['2017年', '11月', '21日', '全国', '中小企业', '股份转让', '系统', '有限', '责任', '公司', '出具', '同意', '江苏', '鸿基', '节能', '新技术', '股份', '有限公司', '终止', '股票', '全国', '中小企业', '股份转让', '系统', '挂牌', '函', '股转', '系统函', '2017]6631号', '同意', '鸿基', '节能', '股票', '2017年', '11月', '24日', '全国', '中小企业', '股份', '转让', '系统', '终止', '挂牌'], ['同日', '鸿基', '节能', '出具', '江苏', '鸿基', '节能', '新', '技术', '股份', '有限公司', '股票', '终止', '挂牌', '公告', '鸿基', '节能', '2017年', '11月', '24日', '终止', '股票', '挂牌']]\n",
      "[['建筑', '工程', '施工', '环境', '状态', '智能', '监测', '监控', '装置', '研究']]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from zhon.hanzi import punctuation\n",
    "import re\n",
    "from ltp import LTP\n",
    "ltp = LTP()\n",
    "\n",
    "class Custom_segment:\n",
    "    def __init__(self, add_words=None):\n",
    "        if add_words:\n",
    "            ltp.add_words(words=add_words, max_window=max([len(w) for w in add_words]))\n",
    "        self.stopwords = self.get_stopwords()\n",
    "\n",
    "    def get_stopwords(self):\n",
    "        with open('../text_process/baidu_stopwords.txt','r', encoding='utf-8') as f:\n",
    "            content = f.readlines()\n",
    "            stopwords = list(map(str.strip, content))\n",
    "        return  stopwords\n",
    "\n",
    "    def cut_sentence(self, text):\n",
    "        sents = ltp.sent_split([text])\n",
    "        return sents\n",
    "\n",
    "    def tokenizers(self, sents):\n",
    "        segs = ltp.seg(sents)[0]\n",
    "        rem_p_segments = map(self.remove_punctuation, segs)\n",
    "        rem_sw_segments = map(self.del_stopwords, rem_p_segments)\n",
    "        return list(rem_sw_segments)\n",
    "\n",
    "    def remove_punctuation(self, words):\n",
    "        new_words = [word for word in words if word not in punctuation and word not in string.punctuation]\n",
    "        return new_words     \n",
    "\n",
    "    def del_stopwords(self, words):\n",
    "        new_words = [word for word in words if word not in self.stopwords]\n",
    "        return new_words\n",
    "        \n",
    "    def __call__(self, text, rm_p=False, del_s=False):\n",
    "\n",
    "        sents = self.cut_sentence(text)\n",
    "        segments = self.tokenizers(sents)\n",
    "        return sents, segments\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    text = \"2017年11月21日，全国中小企业股份转让系统有限责任公司出具《关于同意江苏鸿基节能新技术股份有限公司终止股票全国中小企业股份转让系统挂牌的函》“股转系统函 [2017]6631号”，\\\n",
    "        同意鸿基节能股票自2017年11月24日在全国中小企业股份转让系统终止挂牌。同日，鸿基节能出具《关于江苏鸿基节能新技术股份有限公司股票终止挂牌的公告》，\\\n",
    "        鸿基节能决定自2017年11月24日起终止其股票挂牌\"\n",
    "    custom_segment = Custom_segment()\n",
    "    sents, segments = custom_segment(text)\n",
    "    print(f'sents:{sents}, segments:{segments}')\n",
    "    segments = custom_segment.tokenizers(['建筑工程施工环境安全状态智能监测及安全监控装置研究'])\n",
    "    print(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left_segment: ['2017年', '11月', '21日', '全国', '中小企业', '股份', '转让', '系统', '有限', '责任', '公司', '出具', '同意', '江苏', '鸿基', '节能', '新', '技术', '股份', '有限公司', '终止', '股票', '全国', '中小企业', '股份', '转让', '系统', '挂牌', '函', '股转', '系统', '函', '2017', '6631', '号', '同意', '鸿基', '节能', '股票', '2017年', '11月', '24日', '全国', '中小企业', '股份', '转让', '系统', '终止', '挂牌', '同日', '鸿基', '节能', '出具', '江苏', '鸿基', '节能', '新', '技术', '股份', '有限公司', '股票', '终止', '挂牌', '公告', '鸿基', '节能', '2017年', '11月', '24日', '终止', '股票', '挂牌']\n",
      "right_segment: ['同意', '江苏', '鸿基', '节能', '新', '技术', '股份', '有限公司', '终止', '股票', '全国', '中小企业', '股份', '转让', '系统', '挂牌', '函']\n",
      "right_segment2: ['同意', '江苏', '鸿基', '节能', '新', '技术', '股份', '有限公司', '终止', '股票', '全国', '中小企业', '股份', '转让', '系统', '挂牌', '函', '股转', '系统', '函', '2017', '6631', '号']\n",
      "rate: 1.1\n",
      "rate2: 1.0\n",
      "search_str: 关于同意江苏鸿基节能新技术股份有限公司终止股票全国中小企业股份转让系统挂牌的函》\n",
      "pattern_r:同意.*?江苏.*?鸿基.*?节能.*?新.*?技术.*?股份.*?有限公司.*?终止.*?股票.*?全国.*?中小企业.*?转让.*?系统.*?挂牌.*?函.*?股转.*?2017.*?6631.*?号.*?\n",
      "search_str2: 同意江苏鸿基节能新技术股份有限公司终止股票全国中小企业股份转让系统挂牌的函》“股转系统函 [2017]6631号”\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import re\n",
    "\n",
    "class Lexical_analysis:\n",
    "\n",
    "    @staticmethod\n",
    "    def inclusion_rate(left_segment, right_segment, left_sentence, right_sentence):\n",
    "\n",
    "        '''\n",
    "        Calculate the inclusion degree of the intersection of left segment and right segment in right segment\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input: left_segment, right_segment,left_sentence, right_sentence\n",
    "        Returns\n",
    "        ----------\n",
    "        rate: float\n",
    "        '''\n",
    "        if right_sentence in left_sentence:\n",
    "            rate = 1.1\n",
    "        else:\n",
    "            intersection = set(left_segment) & set(right_segment)\n",
    "            # print(intersection)\n",
    "            rate = len(intersection) / len(set(right_segment))\n",
    "        return rate\n",
    "\n",
    "    @staticmethod\n",
    "    def get_search_str(left_segment, right_segment, left_sentence, right_sentence):\n",
    "        '''\n",
    "        find the serach string that use for add footnote in left_sentence \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input: left_sentence, right_sentence， left_segment, right_segment\n",
    "        Returns\n",
    "        ----------\n",
    "        search_str: string\n",
    "        '''    \n",
    "        # Full character matching\n",
    "        if right_sentence in left_sentence:\n",
    "            if re.findall(right_sentence + '(.)', left_sentence) and re.findall(right_sentence + '(.)', left_sentence)[0] in '》”’）'\\\n",
    "                and not right_sentence.endswith('》') :\n",
    "                search_str = re.findall(right_sentence + '.', left_sentence)[0] \n",
    "            else:\n",
    "                search_str = right_sentence\n",
    "        else:\n",
    "            # Calculate the intersection of left_segment and right_segment\n",
    "            intersection = [seg for seg in right_segment if seg in left_segment]\n",
    "\n",
    "            # Remove duplicates and preserving order of elements\n",
    "            unique_intersection_r = sorted(list(dict.fromkeys(intersection).keys()),key=right_segment.index)\n",
    "            unique_intersection_l = sorted(list(dict.fromkeys(intersection).keys()),key=left_segment.index)\n",
    "\n",
    "            # Pick the words as matching words\n",
    "            matching_words_r = sorted(unique_intersection_r, key=right_segment.index)\n",
    "            matching_words_l = sorted(unique_intersection_l, key=left_segment.index)\n",
    "            # search in matching_words right sentence\n",
    "            pattern_r = ''\n",
    "            for mwr in matching_words_r:\n",
    "                pattern_r = pattern_r + mwr + '.*?'\n",
    "            print(f'pattern_r:{pattern_r}' ) #\n",
    "            # search in matching_words left sentence\n",
    "            pattern_l = ''\n",
    "            for mwl in matching_words_l:\n",
    "                pattern_l = pattern_l + mwl + '.*?'                              \n",
    "            f'pattern_l:{pattern_l}' #\n",
    "            if re.findall(pattern_r + '(.)', left_sentence) and \\\n",
    "                re.findall(pattern_r + '(.)', left_sentence)[0] in '》”’）':\n",
    "                search_str = re.findall(pattern_r + '.', left_sentence)[0]\n",
    "            elif re.search(pattern_r + '：“.+?”', left_sentence):\n",
    "                search_str = re.search(pattern_r + '：“.+?”', left_sentence).group()\n",
    "            elif re.findall(pattern_r, left_sentence):\n",
    "                search_str = re.findall(pattern_r, left_sentence)[0]\n",
    "            elif re.findall(pattern_l + '(.)', left_sentence) and \\\n",
    "                re.findall(pattern_l + '(.)', left_sentence)[0] in '》”’）':\n",
    "                search_str = re.findall(pattern_l + '.', left_sentence)[0]\n",
    "            elif re.search(pattern_l + '：“.+?”', left_sentence):\n",
    "                search_str = re.search(pattern_l + '：“.?”', left_sentence).group()\n",
    "            else :\n",
    "                search_str = re.findall(pattern_l, left_sentence)[0] \n",
    "        return search_str \n",
    "\n",
    "class Dependency_Parser:\n",
    "    pass\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "\n",
    "    custom_segment = Custom_segment()\n",
    "    left_sentence = '2017年11月21日，全国中小企业股份转让系统有限责任公司出具《关于同意江苏鸿基节能新技术股份有限公司终止股票全国中小企业股份转让系统挂牌的函》“股转系统函 [2017]6631号”，同意鸿基节能股票自2017年11月24日在全国中小企业股份转让系统终止挂牌。同日，鸿基节能出具《关于江苏鸿基节能新技术股份有限公司股票终止挂牌的公告》，鸿基节能决定自2017年11月24日起终止其股票挂牌'\n",
    "    right_sentence = \"关于同意江苏鸿基节能新技术股份有限公司终止股票全国中小企业股份转让系统挂牌的函\"\n",
    "    right_sentence2 = \"《关于同意江苏鸿基节能新技术股份有限公司终止股票全国中小企业股份转让系统挂牌的函》（股转系统函[2017]6631号）\"\n",
    "\n",
    "    left_segment = custom_segment.tokenizers([left_sentence])[0]\n",
    "    right_segment = custom_segment.tokenizers([right_sentence])[0]\n",
    "    right_segment2 = custom_segment.tokenizers([right_sentence2])[0]\n",
    "    print(f'left_segment: {left_segment}')\n",
    "    print(f'right_segment: {right_segment}')\n",
    "    print(f'right_segment2: {right_segment2}')\n",
    "\n",
    "    rate = Lexical_analysis.inclusion_rate(left_segment, right_segment, left_sentence, right_sentence)\n",
    "    rate2 = Lexical_analysis.inclusion_rate(left_segment, right_segment, left_sentence, right_sentence2)\n",
    "    print(f'rate: {rate}')\n",
    "    print(f'rate2: {rate2}')\n",
    "\n",
    "\n",
    "    search_str = Lexical_analysis.get_search_str(left_segment, right_segment, left_sentence, right_sentence)\n",
    "    print(f'search_str: {search_str}')\n",
    "\n",
    "    search_str2 = Lexical_analysis.get_search_str(left_segment, right_segment2, left_sentence, right_sentence2) \n",
    "    print(f'search_str2: {search_str2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "786fbf537fb358117198a3d18baf5a6bf1d1200912b85f5ae3d59f2b2715c32d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('ltp': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

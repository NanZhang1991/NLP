{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 合并单条json文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 files Merge completed\n"
     ]
    }
   ],
   "source": [
    "import os\r\n",
    "import json\r\n",
    "\r\n",
    "path  = '../data/云测招股说明书三元组1'\r\n",
    "def merge_json(path):\r\n",
    "    for root, dirs, files in os.walk(path):\r\n",
    "        for n, name in enumerate(dirs):\r\n",
    "            dir_path = os.path.join(root, name)\r\n",
    "            new_fn = os.path.join('../data/招股说明书三元组数据集', name + '.json' )\r\n",
    "            with open(new_fn,'w',encoding='utf-8') as new_file:\r\n",
    "                for fn in os.listdir(dir_path):\r\n",
    "                    if fn.endswith('json'):\r\n",
    "                        fie_path = os.path.join(dir_path, fn)\r\n",
    "                        with open(fie_path, encoding='utf-8') as f:\r\n",
    "                            for line in f.readlines():\r\n",
    "                                new_file.write(line + '\\n')\r\n",
    "            if n == len(dirs) - 1 :\r\n",
    "                print(f'{len(dirs)} files Merge completed')\r\n",
    "merge_json(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取json文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\r\n",
    "\r\n",
    "def json_to_df(path, nrows=False):\r\n",
    "    if nrows:\r\n",
    "        df = pd.read_json(path, nrows=1000, lines=True)\r\n",
    "    else:\r\n",
    "        df = pd.read_json(path, lines=True)\r\n",
    "    df = df[['text', 'spo_list']]\r\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size: (901, 3)\n",
      "                               fn  \\\n",
      "28  210609787c0_聚合顺新材料-广发证券(1)(1)   \n",
      "13  210609787c0_聚合顺新材料-广发证券(1)(1)   \n",
      "9   210609787c0_聚合顺新材料-广发证券(1)(1)   \n",
      "24  210609787c0_聚合顺新材料-广发证券(1)(1)   \n",
      "48  210609787c0_聚合顺新材料-广发证券(1)(1)   \n",
      "\n",
      "                                                 text  \\\n",
      "28  傅昌宝　男，1969　年　12　月出生，中国国籍，拥有澳大利亚永久居留权，大专学历。身份证号...   \n",
      "13  2016　年　4　月　6　日，杭州市市场监督管理局向聚合顺颁发了统一社会信用代码为　9133...   \n",
      "9   2018　年　11　月　20　日，天健会计师事务所出具了“天健验[2018]441　号”《验...   \n",
      "24  2015　年　12　月　31　日，杭州萧然会计师事务所出具“杭萧会内变验（2015）　第　1...   \n",
      "48  聚合顺与浙江恒烨之间的交易仅　2019　年　11　月发生过一笔交易，交易价格与同月同型号其他...   \n",
      "\n",
      "                                             spo_list  \n",
      "28  [{'predicate': '出生日期', 'object': '1969　年　12　月'...  \n",
      "13  [{'predicate': '出具日期', 'object': '2016　年　4　月　6...  \n",
      "9   [{'predicate': '出具日期', 'object': '2018　年　11　月　...  \n",
      "24  [{'predicate': '出具日期', 'object': '2015　年　12　月　...  \n",
      "48  [{'predicate': '关联方', 'object': '浙江恒烨', 'subje...  \n"
     ]
    }
   ],
   "source": [
    "dir_path = '../data/招股说明书三元组数据集'\r\n",
    "\r\n",
    "def merge_df(dir_path):\r\n",
    "    total_df = pd.DataFrame()\r\n",
    "    for fn in os.listdir(dir_path):\r\n",
    "        df = json_to_df(os.path.join(dir_path, fn))\r\n",
    "        df_fn = fn[:fn.rfind('.')]\r\n",
    "        df.insert(0, 'fn', df_fn)\r\n",
    "        total_df =  total_df.append(df)\r\n",
    "    print(f'data size: {total_df.shape}') #\r\n",
    "    print(df.sample(5))\r\n",
    "    return total_df\r\n",
    "\r\n",
    "df = merge_df(dir_path)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- 2.6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('--------', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-20 03:22:38.797031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-08-20 03:22:38.817483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-08-20 03:22:38.818738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:923] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(gpus[-1])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee43b18bdf7c1a4ac65a12e5fa87ef7b96ed7d04836ac8559d1db4b30b000de"
  },
  "kernelspec": {
   "display_name": "Python [conda env:py3.7]",
   "language": "python",
   "name": "conda-env-py3.7-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

# 大模型微调的方法
## Freeze方法：

概述：Freeze方法是指在微调过程中冻结模型的一部分或全部参数，只对部分参数进行更新。通常，会将模型的底层（例如预训练模型的初始层）冻结，只微调顶层或新添加的层的参数。
优点：节省计算资源，尤其适用于训练数据较小或计算能力有限的情况下。
缺点：可能导致模型在新任务上的性能略有下降，因为冻结的部分参数可能无法很好地适应新任务的特征。
Lora方法（Layer-wise Rectified Attention）：

## Lora方法
概述：Lora方法是一种改进的微调方法，它在微调时对注意力机制进行调整，使得模型更加关注新任务的特征。通常通过调整模型的注意力分布来实现。
优点：能够更好地适应新任务的特征，提高模型性能。
缺点：可能需要较大规模的数据集和计算资源来训练。
P-Tuning方法（Parameter-wise Tuning）：

## P-Tuning
概述：P-Tuning方法是指对模型的每个参数进行微调，而不是仅针对部分参数。这种方法通常需要较大规模的数据集和计算资源，并且可能需要更多的训练时间。
优点：能够全面调整模型的参数，适应各种任务和特征。
缺点：需要更多的计算资源和训练时间，且对数据集的要求更高。
全量参数微调：

## 全量参数微调
概述：全量参数微调是指对整个模型的所有参数进行微调，包括底层和顶层。这种方法通常适用于训练数据较大、计算资源充足的情况下。
优点：能够充分利用模型的全部参数，更好地适应新任务。
缺点：需要大量的计算资源和训练时间，对数据集的要求较高。
选择合适的微调方法取决于具体的任务要求、可用的计算资源以及数据集的规模和质量等因素。通常可以通过实验比较不同方法的效果，选择最适合当前任务的微调策略
